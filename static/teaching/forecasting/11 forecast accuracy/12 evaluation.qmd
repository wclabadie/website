---
title: "Evaluating forecast accuracy"
author: "BUS 323 Forecasting and Risk Analysis"
format: 
  revealjs:
    incremental: true
    html-math-method: mathjax
editor: visual
---

## Training and test sets
* Accuracy: more than error size
    + Evaluate forecast performance on historical data
    + Separate into **training** and **test** sets
        - Test set: ~20% of the full sample
        - Or at least as large as the forecast horizon

## Training and test sets
* A couple things to note:
    + Good fit with training data $\neq$ good forecast
    + More parameters $\rightarrow$ perfect fit
    + Over-fitting does not yield a good forecast.
* "Test set" aka "holdout set" or "out-of-sample data".

## Subsetting functions
* ```filter()```
    + e.g. To extract all data from 1995 onward:
```{r}
#| echo: true
#| output: true
#| classes: fragment
library(fpp3)
aus_production |> filter(year(Quarter) >= 1995)
```

## Subsetting functions
* ```filter()```
    + Equivalently:
```{r}
#| echo: true
#| output: true
#| classes: fragment
aus_production |> filter_index("1995 Q1" ~ .)
```

## Subsetting functions
* ```slice()```
    + To extract the last 20 observations:
```{r}
#| echo: true
#| output: true
#| classes: fragment
aus_production |>
  slice(n()-19:0)
```

## Subsetting functions
* ```slice()```
    + To subset the first year of data from each ```State``` and ```Industry```:
```{r}
#| echo: true
#| output: true
#| classes: fragment
aus_retail |>
  group_by(State, Industry) |>
  slice(1:12)
```
    
## Forecast errors
* Forecast residuals based on training set. 
* Forecast errors based on test set.
<div class="fragment">
$$
e_{T+h} = y_{T+h} - \widehat{y}_{T+h|T}
$$
</div>
    + We'll use these to evaluate forecast accuracy.

## Scale-dependent errors
* Forecast errors are on the same scale as the time series being forecasted.
    + Any measure of accuracy based on errors will be similarly scaled.
* Two common measures:
<div class="fragment">
$$
\textrm{Mean absolute error: } MAE = \bar{|e_{t}|}
$$
</div>
<div class="fragment">
$$
\textrm{Root mean squared error: } RMSE = \sqrt{\bar{e_{t}}^{2}}
$$
</div>
    + RMSE commonly used as an objective function for forecast methods.

## Percentage errors
* Used to compare performance across time series
* Most common measure:
<div class="fragment">
$$
\textrm{Mean absolute percentage error: } MAPE = \bar{|p_{t}|}
$$
</div>
    + Where $p_{t} = 100 \frac{e_{t}}{y_{t}}$.
* Percentage errors cause problems when $y_{t}$ is small.

## Example: defining train/test
* Use ```aus_production```. Filter such that ```year``` is $\geq$ 1992. Then define the period 1992-2007 as the training set:
```{r}
#| echo: true
#| output: true
#| classes: fragment
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)
beer_train <- recent_production |>
  filter(year(Quarter) <= 2007)
```

## Example: forecasting 
* 10 observations exist in the test set. Produce a mean, naive, seasonal naive, and drift forecast for those observations.
```{r}
#| echo: true
#| output: true
#| classes: fragment
beer_fit <- beer_train |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  )

beer_fc <- beer_fit |>
  forecast(h = 10)
```

## Example: plotting
* Plot all forecasts with ```autoplot()``` and add in observed data:
```{r}
#| echo: true
#| output: false
#| classes: fragment

beer_fc |>
  autoplot(
    aus_production |> filter(year(Quarter) >= 1992),
    level = NULL
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

## Example: plotting
```{r}
#| echo: false
#| output: true
#| classes: fragment

beer_fc |>
  autoplot(
    aus_production |> filter(year(Quarter) >= 1992),
    level = NULL
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

## Example: forecast accuracy
* Use the ```accuracy()``` function. Supply the forecast object (```beer_fc```) and the observed data (```recent_production``)
```{r}
#| echo: true
#| output: true
#| classes: fragment
accuracy(beer_fc, recent_production)
```

## Example: Google stock price
* Use the 2015 Google stock price data previously constructed. We'll produce forecasts for January 2016.
```{r}
#| echo: true
#| output: true
#| classes: fragment
# Re-index based on trading days
google_stock <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) >= 2015) |>
  mutate(day = row_number()) |>
  update_tsibble(index = day, regular = TRUE)
# Filter for training set
google_2015 <- google_stock |> filter(year(Date) == 2015)
# Filter for test set
google_jan_2016 <- google_stock |>
  filter(yearmonth(Date) == yearmonth("2016 Jan"))
```

## Example: Google stock price
* Produce a mean, naive, and drift forecast for January 2016. Plot the historical data and the forecasts.
```{r}
#| echo: true
#| output: false
#| classes: fragment
google_fit <- google_2015 |>
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = RW(Close ~ drift())
  )

google_fc <- google_fit |>
  forecast(google_jan_2016)
```

## Example: Google stock price
* Evaluate each forecast's accuracy.
```{r}
#| echo: true
#| output: true
#| classes: fragment
accuracy(google_fc, google_stock)
```